{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "from utils import SpecialTokenizer, num_tokens_from_string, get_completion, align_prompt2_n_queries\n",
    "import json\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(\"../data/cc_books_clean_whole2_nonum.json\", \"r\"))\n",
    "#cc_book = data[\"cc_libro_I\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heuristic prompt\n",
    "prompt = \"###Instruction:\\n\\nSei un esperto in materia di giurisprudenza. Formula {} domande possibili a partire dal seguente Testo. Le domande devono strettamente riguardare il contenuto del testo e null'altro. Restituisci esclusivamente le domande e null'altro. Numera ogni domanda formulata.\\n\\n###Testo:\\n\"\n",
    "n_questions = 3\n",
    "\n",
    "#model choice prompt\n",
    "#prompt = \"###Instruction:\\n\\nSei un esperto in materia di giurisprudenza. Formula quante più domande possibili a partire dal seguente Testo. Le domande devono strettamente riguardare il contenuto della risposta e null'altro. Restituisci esclusivamente le domande e null'altro. Numera ogni domanda formulata.\\n\\n###Testo:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [17:10<00:00,  2.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# #trial on book 1\n",
    "# tokenizer = SpecialTokenizer()\n",
    "# query_passage = []\n",
    "# data = cc_book\n",
    "# for k in tqdm(data):\n",
    "#     p = align_prompt2_n_queries(data[k], tokenizer, prompt)\n",
    "#     answ = get_completion(p + data[k], \"gpt-4o\")\n",
    "#     #loop over the generated queries and separate them\n",
    "#     for query in answ.split(\"\\n\"):\n",
    "#         query_passage.append((k, query, data[k]))\n",
    "# #build a dataframe\n",
    "# columns = [\"Articolo di riferimento\", \"Query\", \"Passage\"]\n",
    "# df = pd.DataFrame(query_passage, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cc_libro_II', 'cc_libro_III', 'cc_libro_IV', 'cc_libro_V', 'cc_libro_VI'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import  islice\n",
    "data = json.load(open(\"../data/cc_books_clean_whole2_nonum.json\", \"r\"))\n",
    "data = dict(islice(data.items(), 1, None))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cc_libro_II...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 345/345 [11:25<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved df for book2\n",
      "Processing cc_libro_III...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [13:18<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved df for book3\n",
      "Processing cc_libro_IV...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 888/888 [27:08<00:00,  1.83s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved df for book4\n",
      "Processing cc_libro_V...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 623/623 [22:43<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved df for book5\n",
      "Processing cc_libro_VI...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [09:59<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved df for book6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#complete loop over all the books\n",
    "tokenizer = SpecialTokenizer()\n",
    "dfs = []\n",
    "#process book\n",
    "for n, book in enumerate(data, 2):\n",
    "    print(f\"Processing {book}...\\n\")\n",
    "    query_passage = []\n",
    "    #process articles\n",
    "    for k in tqdm(data[book]):\n",
    "        try:\n",
    "            p = align_prompt2_n_queries(data[book][k], tokenizer, prompt)\n",
    "            answ = get_completion(p + data[book][k], \"gpt-4o\")\n",
    "            \n",
    "            #loop over the generated queries and separate them\n",
    "            for query in answ.split(\"\\n\"):\n",
    "                query_passage.append((k, query, data[book][k].replace(\"\\n\", \" \")))\n",
    "        except:\n",
    "            print(k)\n",
    "            \n",
    "    #build and save a dataframe for each book\n",
    "    columns = [\"Articolo di riferimento\", \"Query\", \"Passage\"]\n",
    "    df = pd.DataFrame(query_passage, columns = columns)\n",
    "    df.to_csv(f\"../results/cc_book{n}_qa.tsv\", sep = \"\\t\")\n",
    "    print(f\"Saved df for book{n}\")\n",
    "    dfs.append(df)\n",
    "    assert len(df[\"Articolo di riferimento\"].unique()) == len(data[book]), \"Number of unique articles does not match number of inputs\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115, 3)\n",
      "(874, 3)\n",
      "(949, 3)\n",
      "(2116, 3)\n",
      "(2132, 3)\n",
      "(890, 3)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"../qa_results/cc_book1_qa.tsv\", sep = \"\\t\")\n",
    "dfs.insert(0, df1)\n",
    "for n,i in enumerate(dfs,1):\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results/all_qa_dfs.pkl\", \"wb\") as outfile:\n",
    "    pickle.dump(dfs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 queries contains ' secondo il testo?' to be removed\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for e,i in enumerate(dfs):\n",
    "    for p in dfs[e][\"Query\"].tolist():\n",
    "        if re.search(r\" secondo il testo\\?\", p):\n",
    "            count+=1\n",
    "print(f\"{count} queries contains ' secondo il testo?' to be removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.read_csv(\"cc_book1_qa.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3[\"Passage\"] = df3[\"Passage\"].apply(lambda x: x.replace(\"\\n\", \" \"))\n",
    "# df.to_csv(\"cc_book1_qa2.tsv\", sep = \"\\t\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"cc_book1_qa.tsv\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:48<00:00,  4.83s/it]\n"
     ]
    }
   ],
   "source": [
    "# df = generate_queries(cc_book, prompt = prompt, subset = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc_dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
